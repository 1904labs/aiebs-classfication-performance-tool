from constants import *
import analysis, logs
import llm_prompts, llm_prompts_openai
import llm_responses

import os
import traceback

import boto3
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import streamlit as st


st.set_page_config(
    page_title=f'LLM Classification Evaluator',
    page_icon='',
    menu_items={
        'Get Help': 'mailto:mpedigo@1904labs.com',
        'About': '1904labs LLM Classification Demo, written by Mark Pedigo'
    },
    layout='wide'
)

with st.sidebar:
    # Framework 
    framework_selection = st.selectbox('Framework', (FRAMEWORK_BEDROCK, FRAMEWORK_OPENAI))

    # Model
    opeanai_models = (MODEL_OPENAI_GPT_3_5_TURBO,)
    bedrock_models = (MODEL_BEDROCK_COHERE_V14,
                      MODEL_BEDROCK_LLAMA2_70B_CHAT_V1,
                    #   MODEL_BEDROCK_JURASSIC2_MID_V1,
                    #   MODEL_BEDROCK_JURASSIC2_ULTRA_V1,
                    #   MODEL_BEDROCK_TITAN_LITE_V1,
                      )

    if framework_selection == FRAMEWORK_OPENAI:
        model_selection = st.selectbox('Model', opeanai_models)
    else:
        model_selection = st.selectbox('Model', bedrock_models)

    # Nbr of runs for this submission
    nbr_of_runs = st.number_input('Number of times to run classification', min_value=1, max_value=10)
    
    # Submit button
    submit = st.button('Submit', type='secondary')

    # Bedrock login, for convenience
    bedrock_login_submit = None
    if framework_selection == FRAMEWORK_BEDROCK:
        st.markdown("***") 
        st.write('Timed/logged out of Bedrock?')   
        bedrock_login_submit = st.button('Bedrock Login', type='secondary')

    if bedrock_login_submit:
        aws_login = 'aws sso login --profile ' + AWS_PROFILE_NAME
        os.system(aws_login)
        bedrock_login_submit = None


# Page header
company_header = '<h1>LLM Intent Classification Evaluator</h1>'
blurb_header = '''This program calculates the accuracy of LLM classifications.
* **Prompt**. The prompt used by the LLM model to classify the data points.
* **Input Data**. Contents of the input data file used to test the classification.
* **Category Predictions**. A listing of the model's predicted classification labels for this data set.
* **Accuracy of Predictions**. The accuracy of the model's predicated labels vs. actual labels.
* **Logs**. Logs generated by this submission.
* **Compare Submissions**. Comparative analysis of different submissions chosen by the user.
'''
st.markdown(company_header, unsafe_allow_html=True)
st.divider()
st.markdown(blurb_header, unsafe_allow_html=True)
st.divider()

# Tabs
input_data_tab, prompt_tab, labels_tab, label_accuracy, logs_tab, compare_tab, model_info_tab = st.tabs(['Input Data',
                                                                                                         'Prompt',
                                                                                                         'Classification Predictions',
                                                                                                         'Accuracy of Classifications',
                                                                                                         'Logs',
                                                                                                         'Compare Submissions',
                                                                                                         'Model Info',])
tabs = [input_data_tab, prompt_tab, labels_tab, label_accuracy, logs_tab, compare_tab, model_info_tab]

# Content    
prompt = llm_prompts.prompt_dispatcher(framework_selection,
                                       model_selection)

# Reference tabs
with input_data_tab:
    input_data = llm_prompts_openai.user_prompt_openai(DATA_FILE)
    st.write(input_data)

with prompt_tab:
    st.write(prompt)

with model_info_tab:
    info_string = '''\
    Llama2: https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md\n
    Cohere: https://docs.cohere.com/docs\n
    Titan: https://aws.amazon.com/machine-learning/responsible-machine-learning/titan-text/\n
    Jurassic2: https://docs.ai21.com/docs/jurassic-2-models\n 
    '''
    st.markdown(info_string)

# Compare logs of previous runs
with compare_tab:
    uploaded_files = st.file_uploader("Choose log files", accept_multiple_files=True)
    if uploaded_files != []:
        logs_analysis_df = analysis.get_logs_analysis_df(uploaded_files)

        heatmap_df, bedrock_df, openai_df = analysis.analyze_df(logs_analysis_df)
        heatmap_df = heatmap_df.transpose()
        
        # Overall accuracy
        st.markdown(f'<h2>Accuracy by API and Intent</h2>', unsafe_allow_html=True)
        st.write('Percentage of intents correctly predicted by API')
        
        ## plot
        ax = sns.heatmap(heatmap_df, cmap=sns.color_palette("ch:start=.2,rot=-.3", as_cmap=True), annot=True, fmt='d')
        ax.figure.set_figheight(2)
        ax.set(xlabel='', ylabel='')
        plt.yticks(rotation=0)
        st.pyplot(ax.get_figure())

        # reports
        st.markdown(f'<h3>Detail - AWS Bedrock</h3>', unsafe_allow_html=True)
        st.write('Intents correctly/incorrectly predicted')
        st.write(bedrock_df)

        st.markdown(f'<h3>OpenAI by intent</h3>', unsafe_allow_html=True)
        st.write('Intents correctly/incorrectly predicted')
        st.write(openai_df)

# Has user submitted?
if not submit:
    # Info screens
    with labels_tab: st.write('← Please choose options and submit.')
    with label_accuracy: st.write('← Please choose options and submit.')
    with logs_tab: st.write('← Please choose options and submit.')
else:
    ##############################
    # LLM response and analysis
    ##############################
    with labels_tab:

        # Framework creds
        openai_key = None
        aws_client = None
        if framework_selection == FRAMEWORK_OPENAI:
            try:
                openai_key = os.getenv('OPENAI_API_KEY')
            except Exception as e:
                st.info(f'OpenAI Error. {e}')
                st.info(traceback.format_exc())
                st.stop()
        else:  # Bedrock
            try:
                session = boto3.Session(profile_name=AWS_PROFILE_NAME)
                aws_client = session.client('bedrock-runtime', region_name=AWS_REGION_NAME)
            except Exception as e:
                st.info(f'AWS ERROR. {e}')
                st.info(traceback.format_exc())
                st.stop()

        # LLM response, analytics, logs
        nbr_correct_total = 0
        nbr_nonerror_loops = 0
        nbr_total_no_nan = 0
        for i in range(nbr_of_runs):
            st.markdown(f'<h3>Run #{i + 1}</h3>', unsafe_allow_html=True)

            try:
                with st.spinner('Thinking...'):
                    response = llm_responses.response_dispatcher(prompt,
                                                                 framework_selection,
                                                                 model_selection,
                                                                 openai_key=openai_key,
                                                                 aws_client=aws_client)
                    st.chat_message('assistant').write(response)
            except Exception as e:
                st.info(traceback.format_exc())
                st.stop()

            # Logs
            with logs_tab:
                try:
                    log_report_file_name, log_report, date = logs.make_log_report(DATA_FILE, response, framework_selection, model_selection)
                    st.info(f'Log for API = {framework_selection}, model = {model_selection}, date = {date}')
                    st.text(log_report)
                except Exception as e:
                    st.info(traceback.format_exc())
                    st.stop()

            # Analysis
            with label_accuracy:
                try:
                    st.markdown(f'\n\n\n<h3>Run {i + 1}</h3>', unsafe_allow_html=True)
                    prediction_error, nbr_correct, nbr_total, prediction_percentage = analysis.get_prediction_errors(response, DATA_FILE)
                    # prediction_error, nbr_correct, nbr_total, prediction_percentage = logs.get_prediction_errors(log_report_file_name)
                    if np.isnan(nbr_correct):
                        st.markdown('ERROR\n\n')
                    else:
                        st.markdown(f'Prediction accuracy = {nbr_correct} / {nbr_total} = {prediction_percentage:,.0f}%.')
                        nbr_correct_total += nbr_correct
                        nbr_nonerror_loops += 1
                        nbr_total_no_nan = nbr_total
                        if 0 <= prediction_percentage < 100:
                            st.markdown(prediction_error, unsafe_allow_html=True)
                except Exception as e:
                    st.info(traceback.format_exc())
                    st.stop()        

        # Accuracy over all runs
        with label_accuracy:
            st.markdown(f'<h3>Overall accuracy</h3>', unsafe_allow_html=True)
            accuracy = 100 * nbr_correct_total / (nbr_total_no_nan * nbr_nonerror_loops)
            st.markdown(f'Overall accuracy = {nbr_correct_total} / {(nbr_total_no_nan * nbr_nonerror_loops)} = {accuracy:.0f}%.')
